{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# CSC14120 - Parallel Programming Final Project\n",
                "# Autoencoder-based Feature Learning for CIFAR-10 Classification\n",
                "\n",
                "---\n",
                "\n",
                "**Team:** Team 18\n",
                "\n",
                "**Video Presentation:** [YouTube Link - Unlisted]"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "# Section 1: Problem Description\n",
                "\n",
                "## 1.1 Problem Statement\n",
                "\n",
                "Unsupervised feature learning using Autoencoder for CIFAR-10 image classification with GPU acceleration.\n",
                "\n",
                "| Stage | Description | Output |\n",
                "|-------|-------------|--------|\n",
                "| Stage 1 | Train Autoencoder (unsupervised) | 8,192-dim features |\n",
                "| Stage 2 | Train SVM on extracted features | 10-class classification |\n",
                "\n",
                "**Motivation:** CPU training takes hours due to compute-intensive convolution operations. GPU parallelization targets <10 minute training with >20x speedup.\n",
                "\n",
                "## 1.2 CIFAR-10 Dataset\n",
                "\n",
                "| Specification | Value |\n",
                "|---------------|-------|\n",
                "| Image size | 32x32x3 (RGB) |\n",
                "| Training set | 50,000 images |\n",
                "| Test set | 10,000 images |\n",
                "| Classes | 10 (airplane, automobile, bird, cat, deer, dog, frog, horse, ship, truck) |\n",
                "| Data format | Binary files, uint8 pixels normalized to [0,1] |"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Setup\n",
                "from google.colab import userdata\n",
                "import os\n",
                "\n",
                "repos = \"https://github.com/QuackPhuc/AutoEncoder-CUDA.git\"\n",
                "\n",
                "if not os.path.exists('/content/AutoEncoder-CUDA'):\n",
                "    !git clone --recursive {repos}\n",
                "\n",
                "%cd /content/AutoEncoder-CUDA\n",
                "!chmod +x scripts/download_cifar10.sh\n",
                "!scripts/download_cifar10.sh"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Dataset samples visualization\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "\n",
                "def load_cifar10_batch(file_path):\n",
                "    with open(file_path, 'rb') as f:\n",
                "        data = np.frombuffer(f.read(), dtype=np.uint8)\n",
                "    data = data.reshape(-1, 3073)\n",
                "    labels = data[:, 0]\n",
                "    images = data[:, 1:].reshape(-1, 3, 32, 32).transpose(0, 2, 3, 1)\n",
                "    return images, labels\n",
                "\n",
                "images, labels = load_cifar10_batch('/content/AutoEncoder-CUDA/data/data_batch_1.bin')\n",
                "class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
                "\n",
                "fig, axes = plt.subplots(2, 10, figsize=(14, 3))\n",
                "fig.suptitle('CIFAR-10 Dataset Samples', fontsize=11, fontweight='bold')\n",
                "for class_idx in range(10):\n",
                "    class_images = images[labels == class_idx]\n",
                "    for row in range(2):\n",
                "        ax = axes[row, class_idx]\n",
                "        ax.imshow(class_images[row])\n",
                "        ax.axis('off')\n",
                "        if row == 0:\n",
                "            ax.set_title(class_names[class_idx], fontsize=8)\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1.3 Autoencoder Architecture\n",
                "\n",
                "```text\n",
                "INPUT (32x32x3)\n",
                "    |\n",
                "[ENCODER]\n",
                "    Conv2D(256, 3x3, pad=1) + ReLU -> (32x32x256)   # 7,168 params\n",
                "    MaxPool2D(2x2)                 -> (16x16x256)\n",
                "    Conv2D(128, 3x3, pad=1) + ReLU -> (16x16x128)   # 295,040 params\n",
                "    MaxPool2D(2x2)                 -> (8x8x128)\n",
                "    |\n",
                "LATENT REPRESENTATION (8x8x128 = 8,192 dimensions)\n",
                "    |\n",
                "[DECODER]\n",
                "    Conv2D(128, 3x3, pad=1) + ReLU -> (8x8x128)     # 147,584 params\n",
                "    Upsample2D(2x2)                -> (16x16x128)\n",
                "    Conv2D(256, 3x3, pad=1) + ReLU -> (16x16x256)   # 295,168 params\n",
                "    Upsample2D(2x2)                -> (32x32x256)\n",
                "    Conv2D(3, 3x3, pad=1)          -> (32x32x3)     # 6,915 params\n",
                "    |\n",
                "OUTPUT (32x32x3)\n",
                "```\n",
                "\n",
                "**Total Parameters:** 751,875 (all trainable)\n",
                "\n",
                "## 1.4 Performance Targets\n",
                "\n",
                "| Metric | Target |\n",
                "|--------|--------|\n",
                "| Autoencoder training time | < 10 minutes |\n",
                "| Feature extraction time | < 20 seconds (60K images) |\n",
                "| Test classification accuracy | 60-65% |\n",
                "| GPU speedup over CPU | > 20x |"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "# Section 2: Implementation Phases"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Build project\n",
                "%cd /content/AutoEncoder-CUDA\n",
                "!chmod +x build.sh run.sh\n",
                "!./build.sh --clean"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Phase 2.1: CPU Baseline\n",
                "\n",
                "**Objective:** Implement complete autoencoder on CPU to establish performance baseline.\n",
                "\n",
                "**Implementation Details:**\n",
                "\n",
                "| Component | Description |\n",
                "|-----------|-------------|\n",
                "| Data Pipeline | CIFAR-10 binary loader with normalization to [0,1] |\n",
                "| Conv2D | 6 nested loops: output channels, height, width, kernel h/w, input channels |\n",
                "| ReLU | Element-wise max(0, x) |\n",
                "| MaxPool2D | 2x2 window max with stride 2 |\n",
                "| Upsample2D | Nearest neighbor interpolation |\n",
                "| Loss | MSE between input and reconstruction |\n",
                "\n",
                "**Key Code: Conv2D Forward (src/cpu/layers/conv2d.cpp)**\n",
                "```cpp\n",
                "std::vector<float> Conv2D::forward(const std::vector<float>& input, int H, int W) {\n",
                "    m_outH = (H + 2 * m_padding - m_kernelSize) / m_stride + 1;\n",
                "    m_outW = (W + 2 * m_padding - m_kernelSize) / m_stride + 1;\n",
                "    std::vector<float> output(m_outH * m_outW * m_outChannels, 0.0f);\n",
                "    \n",
                "    for (int oc = 0; oc < m_outChannels; ++oc) {\n",
                "        for (int oh = 0; oh < m_outH; ++oh) {\n",
                "            for (int ow = 0; ow < m_outW; ++ow) {\n",
                "                float sum = m_bias[oc];\n",
                "                for (int kh = 0; kh < m_kernelSize; ++kh)\n",
                "                    for (int kw = 0; kw < m_kernelSize; ++kw)\n",
                "                        for (int ic = 0; ic < m_inChannels; ++ic) {\n",
                "                            int ih = oh * m_stride + kh - m_padding;\n",
                "                            int iw = ow * m_stride + kw - m_padding;\n",
                "                            sum += getPaddedValue(input, ih, iw, ic) * m_weights[wIdx];\n",
                "                        }\n",
                "                output[(oh * m_outW + ow) * m_outChannels + oc] = sum;\n",
                "            }\n",
                "        }\n",
                "    }\n",
                "    return output;\n",
                "}\n",
                "```\n",
                "\n",
                "**Key Code: Training Loop**\n",
                "```cpp\n",
                "for (int epoch = 0; epoch < epochs; ++epoch) {\n",
                "    for (int batch = 0; batch < numBatches; ++batch) {\n",
                "        auto batchData = dataset.getBatch(batch, batchSize);\n",
                "        auto output = model.forward(batchData);       // Encoder + Decoder\n",
                "        float loss = mseLoss.compute(batchData, output);\n",
                "        model.backward(mseLoss.gradient());           // Backprop\n",
                "        model.updateWeights(learningRate);            // SGD\n",
                "    }\n",
                "}\n",
                "```\n",
                "\n",
                "**Key Takeaway:** 6 nested loops in convolution account for ~90% of compute time. This is the primary optimization target for GPU."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import subprocess\n",
                "import time\n",
                "import re\n",
                "\n",
                "results = {}\n",
                "EPOCHS, SAMPLES = 3, 100  # Reduced params for demo\n",
                "\n",
                "print(f\"Running CPU Baseline (epochs={EPOCHS}, samples={SAMPLES})...\")\n",
                "start = time.time()\n",
                "r = subprocess.run(['./build/bin/autoencoder_cpu', '--epochs', str(EPOCHS), '--samples', str(SAMPLES)],\n",
                "                   capture_output=True, text=True, cwd='/content/AutoEncoder-CUDA')\n",
                "cpu_time = time.time() - start\n",
                "\n",
                "loss_match = re.findall(r'Loss: ([0-9.]+)', r.stdout)\n",
                "cpu_loss = float(loss_match[-1]) if loss_match else 0.0\n",
                "results['CPU'] = {'time': cpu_time, 'loss': cpu_loss}\n",
                "\n",
                "print(f\"CPU Baseline: {cpu_time:.2f}s, Loss: {cpu_loss:.6f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Phase 2.2: GPU Basic (Naive)\n",
                "\n",
                "**Objective:** Port CPU code to GPU with basic parallelization. Verify correctness against CPU baseline.\n",
                "\n",
                "**Parallelization Strategy:**\n",
                "\n",
                "| Layer | Thread Mapping | Grid/Block Config |\n",
                "|-------|----------------|-------------------|\n",
                "| Conv2D | 1 thread = 1 output element | grid((N*H*W*C+255)/256), block(256) |\n",
                "| ReLU | 1 thread = 1 element | Same as above |\n",
                "| MaxPool | 1 thread = 1 output pixel | grid((N*outH*outW*C+255)/256), block(256) |\n",
                "| Upsample | 1 thread = 1 output pixel | Same as above |\n",
                "\n",
                "**Key Code: Naive Conv2D Kernel (src/gpu/kernels/forward/conv2d.cu)**\n",
                "```cpp\n",
                "__global__ void conv2dForwardKernel(\n",
                "    const float* input, const float* weights, const float* bias, float* output,\n",
                "    int batch, int inH, int inW, int inC, int outH, int outW, int outC,\n",
                "    int kernelSize, int padding, int stride\n",
                ") {\n",
                "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
                "    if (idx >= batch * outH * outW * outC) return;\n",
                "    \n",
                "    // Decode NHWC index\n",
                "    int c = idx % outC;\n",
                "    int w = (idx / outC) % outW;\n",
                "    int h = (idx / (outC * outW)) % outH;\n",
                "    int n = idx / (outC * outW * outH);\n",
                "    \n",
                "    float sum = 0.0f;\n",
                "    for (int kh = 0; kh < kernelSize; kh++)\n",
                "        for (int kw = 0; kw < kernelSize; kw++)\n",
                "            for (int ic = 0; ic < inC; ic++) {\n",
                "                int in_h = h * stride + kh - padding;\n",
                "                int in_w = w * stride + kw - padding;\n",
                "                if (in_h >= 0 && in_h < inH && in_w >= 0 && in_w < inW)\n",
                "                    sum += input[...] * weights[...];\n",
                "            }\n",
                "    output[idx] = sum + bias[c];\n",
                "}\n",
                "```\n",
                "\n",
                "**Key Takeaway:** Basic parallelization achieves significant speedup but is memory-bandwidth bound due to uncoalesced global memory accesses in convolution kernel."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(f\"Running GPU Naive (epochs={EPOCHS}, samples={SAMPLES})...\")\n",
                "start = time.time()\n",
                "r = subprocess.run(['./build/bin/autoencoder_gpu', '--gpu-version', '1', '--epochs', str(EPOCHS), '--samples', str(SAMPLES)],\n",
                "                   capture_output=True, text=True, cwd='/content/AutoEncoder-CUDA')\n",
                "gpu_naive_time = time.time() - start\n",
                "\n",
                "loss_match = re.findall(r'Loss: ([0-9.]+)', r.stdout)\n",
                "results['GPU-naive'] = {'time': gpu_naive_time, 'loss': float(loss_match[-1]) if loss_match else 0.0}\n",
                "\n",
                "print(f\"GPU Naive: {gpu_naive_time:.2f}s, Speedup: {results['CPU']['time']/gpu_naive_time:.1f}x\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Phase 2.3: GPU Optimized v1 (Memory Optimizations)\n",
                "\n",
                "**Optimization Focus:** Reduce global memory accesses through shared memory tiling.\n",
                "\n",
                "**Optimizations Applied:**\n",
                "\n",
                "| Technique | Description | Expected Benefit |\n",
                "|-----------|-------------|------------------|\n",
                "| Shared Memory Tiling | Load 18x18 input tile per 16x16 output tile | ~9x reduction in global reads |\n",
                "| Constant Memory | Store bias values in constant memory | Fast broadcast to all threads |\n",
                "| Memory Coalescing | NHWC layout ensures consecutive threads access consecutive addresses | Better memory bandwidth |\n",
                "\n",
                "**Key Code: Shared Memory Conv2D (src/gpu/kernels/forward/conv2d.cu)**\n",
                "```cpp\n",
                "#define TILE_SIZE 16\n",
                "#define HALO_SIZE 1  // For 3x3 kernel with padding=1\n",
                "#define SHARED_TILE_SIZE (TILE_SIZE + 2 * HALO_SIZE)  // 18\n",
                "\n",
                "__global__ void conv2dForwardSharedKernel(\n",
                "    const float* __restrict__ input,\n",
                "    const float* __restrict__ weights,\n",
                "    float* __restrict__ output, ...\n",
                ") {\n",
                "    extern __shared__ float s_tile[];  // 18x18 tile\n",
                "    \n",
                "    // Cooperative loading: all threads in block load input tile\n",
                "    for (int t = 0; t < tilesNeeded; t++) {\n",
                "        int loadIdx = t * (TILE_SIZE * TILE_SIZE) + threadId;\n",
                "        if (loadIdx < SHARED_TILE_SIZE * SHARED_TILE_SIZE)\n",
                "            s_tile[loadIdx] = loadFromGlobal(...);\n",
                "    }\n",
                "    __syncthreads();\n",
                "    \n",
                "    // Compute using shared memory (no global reads in inner loop)\n",
                "    for (int kh = 0; kh < 3; kh++)\n",
                "        for (int kw = 0; kw < 3; kw++)\n",
                "            sum += s_tile[...] * weights[...];\n",
                "    \n",
                "    output[outIdx] = sum + d_constBias[oc];\n",
                "}\n",
                "```\n",
                "\n",
                "**Analysis:** Shared memory reduces global memory traffic significantly. The 18x18 tile is reused by all 256 threads in a block, amortizing the load cost."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(f\"Running GPU Opt v1 (epochs={EPOCHS}, samples={SAMPLES})...\")\n",
                "start = time.time()\n",
                "r = subprocess.run(['./build/bin/autoencoder_gpu', '--gpu-version', '2', '--epochs', str(EPOCHS), '--samples', str(SAMPLES)],\n",
                "                   capture_output=True, text=True, cwd='/content/AutoEncoder-CUDA')\n",
                "gpu_v1_time = time.time() - start\n",
                "\n",
                "loss_match = re.findall(r'Loss: ([0-9.]+)', r.stdout)\n",
                "results['GPU-v1'] = {'time': gpu_v1_time, 'loss': float(loss_match[-1]) if loss_match else 0.0}\n",
                "\n",
                "print(f\"GPU Opt v1: {gpu_v1_time:.2f}s, Speedup vs CPU: {results['CPU']['time']/gpu_v1_time:.1f}x, vs Naive: {results['GPU-naive']['time']/gpu_v1_time:.2f}x\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Phase 2.4: GPU Optimized v2 (Kernel Fusion)\n",
                "\n",
                "**Optimization Focus:** Eliminate intermediate memory writes through kernel fusion.\n",
                "\n",
                "**Optimizations Applied:**\n",
                "\n",
                "| Technique | Description | Expected Benefit |\n",
                "|-----------|-------------|------------------|\n",
                "| Conv+ReLU Fusion | Apply ReLU inline after convolution | Eliminate 1 global write + 1 global read per element |\n",
                "| Loop Unrolling | #pragma unroll for 3x3 kernel loops | Reduce loop overhead |\n",
                "| Vectorized Access | float4 loads where applicable | 4x bandwidth efficiency |\n",
                "\n",
                "**Key Code: Fused Conv+ReLU Kernel (src/gpu/kernels/forward/conv2d.cu)**\n",
                "```cpp\n",
                "__global__ void conv2dForwardSharedReluKernel(...) {\n",
                "    extern __shared__ float s_tile[];\n",
                "    // ... shared memory loading ...\n",
                "    \n",
                "    float sum = 0.0f;\n",
                "    #pragma unroll\n",
                "    for (int kh = 0; kh < 3; kh++) {\n",
                "        #pragma unroll\n",
                "        for (int kw = 0; kw < 3; kw++) {\n",
                "            sum += s_tile[sharedY * SHARED_TILE_SIZE + sharedX] * weights[wIdx];\n",
                "        }\n",
                "    }\n",
                "    \n",
                "    // Fused: add bias and apply ReLU in single operation\n",
                "    sum += d_constBias[oc];\n",
                "    sum = fmaxf(0.0f, sum);  // ReLU inline\n",
                "    output[outIdx] = sum;\n",
                "}\n",
                "```\n",
                "\n",
                "**Analysis:** Fusion eliminates separate ReLU kernel launch and associated memory traffic. Diminishing returns observed as we approach memory bandwidth limits."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(f\"Running GPU Opt v2 (epochs={EPOCHS}, samples={SAMPLES})...\")\n",
                "start = time.time()\n",
                "r = subprocess.run(['./build/bin/autoencoder_gpu', '--gpu-version', '3', '--epochs', str(EPOCHS), '--samples', str(SAMPLES)],\n",
                "                   capture_output=True, text=True, cwd='/content/AutoEncoder-CUDA')\n",
                "gpu_v2_time = time.time() - start\n",
                "\n",
                "loss_match = re.findall(r'Loss: ([0-9.]+)', r.stdout)\n",
                "results['GPU-v2'] = {'time': gpu_v2_time, 'loss': float(loss_match[-1]) if loss_match else 0.0}\n",
                "\n",
                "print(f\"GPU Opt v2: {gpu_v2_time:.2f}s, Speedup vs CPU: {results['CPU']['time']/gpu_v2_time:.1f}x, vs v1: {results['GPU-v1']['time']/gpu_v2_time:.2f}x\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Phase 2.5: SVM Integration\n",
                "\n",
                "**Objective:** Complete classification pipeline using trained encoder features.\n",
                "\n",
                "**Implementation Details:**\n",
                "\n",
                "| Component | Description |\n",
                "|-----------|-------------|\n",
                "| Feature Extraction | Run encoder forward pass, output 8192-dim latent vector |\n",
                "| SVM Library | ThunderSVM (GPU-accelerated) |\n",
                "| Kernel | RBF (Radial Basis Function) |\n",
                "| Hyperparameters | C=1.0, gamma=auto (1/num_features) |\n",
                "\n",
                "**Key Code: Feature Extraction**\n",
                "```cpp\n",
                "std::vector<float> extractFeatures(GPUAutoencoder& encoder, \n",
                "                                   const std::vector<float>& images) {\n",
                "    // Run encoder only (no decoder)\n",
                "    return encoder.encodeOnly(images);  // Returns (N, 8192) features\n",
                "}\n",
                "```\n",
                "\n",
                "---\n",
                "\n",
                "### Option A: Use Pre-trained Weights (Recommended for Demo)\n",
                "\n",
                "Pre-trained weights are included in the repository. This skips the 2-hour training time."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Option A: Evaluate using pre-trained weights\n",
                "os.makedirs('/content/AutoEncoder-CUDA/results', exist_ok=True)\n",
                "\n",
                "encoder_weights = '/content/AutoEncoder-CUDA/checkpoints/encoder.weights'\n",
                "svm_model = '/content/AutoEncoder-CUDA/checkpoints/svm.bin'\n",
                "\n",
                "if os.path.exists(encoder_weights) and os.path.exists(svm_model):\n",
                "    print(\"Using pre-trained weights for evaluation...\")\n",
                "    r = subprocess.run(\n",
                "        ['./build/bin/autoencoder_inference', \n",
                "         '--encoder-weights', encoder_weights,\n",
                "         '--svm-model', svm_model,\n",
                "         '--evaluate-only'],\n",
                "        capture_output=True, text=True, cwd='/content/AutoEncoder-CUDA')\n",
                "    print(r.stdout)\n",
                "    if r.stderr:\n",
                "        print(\"Errors:\", r.stderr)\n",
                "else:\n",
                "    print(\"Pre-trained weights not found. Run Option B below to train from scratch.\")\n",
                "    print(f\"  Encoder: {encoder_weights} - {'Found' if os.path.exists(encoder_weights) else 'NOT FOUND'}\")\n",
                "    print(f\"  SVM: {svm_model} - {'Found' if os.path.exists(svm_model) else 'NOT FOUND'}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Option B: Train from Scratch (Takes ~2 hours)\n",
                "\n",
                "Uncomment the cells below to train the full pipeline from scratch."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# # Option B: Full training from scratch\n",
                "# # Uncomment to run (takes ~2 hours on Tesla T4)\n",
                "\n",
                "# FULL_EPOCHS = 20\n",
                "# os.makedirs('/content/AutoEncoder-CUDA/results', exist_ok=True)\n",
                "\n",
                "# print(f\"Step 1: Training Autoencoder (epochs={FULL_EPOCHS}, all 50000 samples)...\")\n",
                "# start = time.time()\n",
                "# r = subprocess.run(\n",
                "#     ['./build/bin/autoencoder_gpu', '--gpu-version', '2', '--epochs', str(FULL_EPOCHS), \n",
                "#      '--samples', '0', '--save-weights', './checkpoints/encoder.weights'],\n",
                "#     capture_output=True, text=True, cwd='/content/AutoEncoder-CUDA')\n",
                "# train_time = time.time() - start\n",
                "# print(r.stdout[-2000:])  # Last 2000 chars\n",
                "# print(f\"\\nAutoencoder training: {train_time:.2f}s ({train_time/60:.1f} min)\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# # Step 2: Train SVM and Evaluate\n",
                "\n",
                "# print(\"Step 2: Training SVM and Evaluating...\")\n",
                "# r = subprocess.run(\n",
                "#     ['./build/bin/autoencoder_inference',\n",
                "#      '--encoder-weights', './checkpoints/encoder.weights',\n",
                "#      '--svm-model', './checkpoints/svm.bin',\n",
                "#      '--train-svm'],\n",
                "#     capture_output=True, text=True, cwd='/content/AutoEncoder-CUDA')\n",
                "# print(r.stdout)\n",
                "# if r.stderr:\n",
                "#     print(\"Errors:\", r.stderr)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "# Section 3: Performance Analysis\n",
                "\n",
                "## 3.1 Benchmark Results\n",
                "\n",
                "**Test Configuration:** epochs=3, samples=100 (Tesla T4 GPU)\n",
                "\n",
                "| Version | Time (s) | Speedup vs CPU | Incremental Speedup | Key Optimization |\n",
                "|---------|----------|----------------|---------------------|------------------|\n",
                "| CPU | 439.97 | 1.0x | - | Baseline (nested loops) |\n",
                "| GPU-naive | 2.64 | 166.7x | 166.7x | Basic parallelization |\n",
                "| GPU-v1 | 1.96 | 224.0x | 1.35x | Shared memory tiling |\n",
                "| GPU-v2 | 1.95 | 226.0x | 1.01x | Kernel fusion |\n",
                "\n",
                "**Full Training (20 epochs, 50K samples):**\n",
                "\n",
                "| Metric | Value |\n",
                "|--------|-------|\n",
                "| Total training time | ~126 min |\n",
                "| Time per epoch | ~6 min |\n",
                "| Feature extraction (60K images) | ~15 seconds |\n",
                "| SVM training | ~5 minutes |"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Performance visualization\n",
                "import matplotlib.pyplot as plt\n",
                "import numpy as np\n",
                "\n",
                "versions = list(results.keys())\n",
                "times = [results[v]['time'] for v in versions]\n",
                "cpu_time = results['CPU']['time']\n",
                "speedups = [cpu_time / t if t > 0 else 0 for t in times]\n",
                "\n",
                "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
                "\n",
                "# Plot 1: Training time (log scale)\n",
                "colors = ['#3498db', '#e74c3c', '#2ecc71', '#f39c12'][:len(versions)]\n",
                "bars = axes[0].bar(versions, times, color=colors)\n",
                "axes[0].set_ylabel('Training Time (s)')\n",
                "axes[0].set_title('Training Time Comparison')\n",
                "axes[0].set_yscale('log')\n",
                "for bar, t in zip(bars, times):\n",
                "    axes[0].text(bar.get_x() + bar.get_width()/2, bar.get_height(), f'{t:.2f}s', \n",
                "                 ha='center', va='bottom', fontsize=9)\n",
                "\n",
                "# Plot 2: Speedup bar chart\n",
                "axes[1].bar(versions, speedups, color=colors)\n",
                "axes[1].set_ylabel('Speedup vs CPU')\n",
                "axes[1].set_title('GPU Speedup')\n",
                "axes[1].axhline(y=20, color='red', linestyle='--', alpha=0.7, label='Target (20x)')\n",
                "axes[1].legend()\n",
                "for i, s in enumerate(speedups):\n",
                "    axes[1].text(i, s + 5, f'{s:.1f}x', ha='center', fontsize=9)\n",
                "\n",
                "# Plot 3: Cumulative speedup line graph\n",
                "axes[2].plot(versions, speedups, 'o-', color='#2ecc71', linewidth=2, markersize=8)\n",
                "axes[2].set_ylabel('Cumulative Speedup')\n",
                "axes[2].set_title('Optimization Progress')\n",
                "axes[2].axhline(y=20, color='red', linestyle='--', alpha=0.7, label='Target (20x)')\n",
                "axes[2].legend()\n",
                "axes[2].grid(True, alpha=0.3)\n",
                "for i, (v, s) in enumerate(zip(versions, speedups)):\n",
                "    axes[2].annotate(f'{s:.1f}x', (i, s), textcoords='offset points', xytext=(0, 10), ha='center')\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Summary table\n",
                "print(f\"{'Version':<15} {'Time (s)':<12} {'Speedup':<12} {'Optimization'}\")\n",
                "print(\"-\" * 65)\n",
                "opts = {\n",
                "    'CPU': 'Baseline (6 nested loops)', \n",
                "    'GPU-naive': 'Basic parallelization', \n",
                "    'GPU-v1': 'NCHW layout + 2D grid + warp shuffle', \n",
                "    'GPU-v2': 'im2col + cuBLAS GEMM'\n",
                "}\n",
                "for v in results:\n",
                "    t = results[v]['time']\n",
                "    s = cpu_time / t if t > 0 else 0\n",
                "    print(f\"{v:<15} {t:<12.2f} {s:<12.1f}x {opts.get(v, '')}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3.2 Reconstruction Quality\n",
                "\n",
                "Visual comparison of original images and autoencoder reconstructions."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Reconstruction visualization (if reconstruction output exists)\n",
                "recon_file = '/content/AutoEncoder-CUDA/results/reconstructions.bin'\n",
                "if os.path.exists(recon_file):\n",
                "    with open(recon_file, 'rb') as f:\n",
                "        recon_data = np.frombuffer(f.read(), dtype=np.float32)\n",
                "    num_samples = min(10, len(recon_data) // (32*32*3))\n",
                "    recon_images = recon_data[:num_samples*32*32*3].reshape(num_samples, 32, 32, 3)\n",
                "    recon_images = np.clip(recon_images, 0, 1)\n",
                "    \n",
                "    fig, axes = plt.subplots(2, num_samples, figsize=(14, 3))\n",
                "    fig.suptitle('Original (top) vs Reconstructed (bottom)', fontsize=11, fontweight='bold')\n",
                "    for i in range(num_samples):\n",
                "        axes[0, i].imshow(images[i] / 255.0)\n",
                "        axes[0, i].axis('off')\n",
                "        axes[1, i].imshow(recon_images[i])\n",
                "        axes[1, i].axis('off')\n",
                "    plt.tight_layout()\n",
                "    plt.show()\n",
                "else:\n",
                "    print(\"Reconstruction file not found. Run training to generate reconstructions.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3.3 Classification Results"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Confusion matrix visualization\n",
                "import pandas as pd\n",
                "\n",
                "confusion_csv = '/content/AutoEncoder-CUDA/results/confusion_matrix.csv'\n",
                "if os.path.exists(confusion_csv):\n",
                "    import seaborn as sns\n",
                "    cm_df = pd.read_csv(confusion_csv, index_col=0)\n",
                "    \n",
                "    plt.figure(figsize=(8, 6))\n",
                "    sns.heatmap(cm_df, annot=True, fmt='d', cmap='Blues', \n",
                "                xticklabels=class_names, yticklabels=class_names)\n",
                "    plt.title('Confusion Matrix')\n",
                "    plt.xlabel('Predicted')\n",
                "    plt.ylabel('True')\n",
                "    plt.tight_layout()\n",
                "    plt.show()\n",
                "    \n",
                "    # Per-class accuracy\n",
                "    cm = cm_df.values\n",
                "    per_class = np.diag(cm) / cm.sum(axis=1) * 100\n",
                "    overall = np.trace(cm) / cm.sum() * 100\n",
                "    \n",
                "    print(f\"\\nOverall Accuracy: {overall:.2f}%\")\n",
                "    print(f\"\\nPer-class Accuracy:\")\n",
                "    for i, name in enumerate(class_names):\n",
                "        print(f\"  {name:<12}: {per_class[i]:.1f}%\")\n",
                "else:\n",
                "    print(\"Confusion matrix not available. Run evaluation first.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Per-class accuracy bar chart\n",
                "if os.path.exists(confusion_csv):\n",
                "    fig, ax = plt.subplots(figsize=(10, 4))\n",
                "    bars = ax.bar(class_names, per_class, color=plt.cm.tab10.colors)\n",
                "    ax.axhline(y=overall, color='red', linestyle='--', linewidth=2, label=f'Overall: {overall:.1f}%')\n",
                "    ax.set_ylabel('Accuracy (%)')\n",
                "    ax.set_title('Per-class Classification Accuracy')\n",
                "    ax.set_ylim(0, 100)\n",
                "    ax.legend()\n",
                "    for bar, acc in zip(bars, per_class):\n",
                "        ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1, \n",
                "                f'{acc:.1f}%', ha='center', fontsize=8)\n",
                "    plt.xticks(rotation=45, ha='right')\n",
                "    plt.tight_layout()\n",
                "    plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "# Section 4: Lessons Learned\n",
                "\n",
                "## 4.1 Technical Insights\n",
                "\n",
                "**CUDA Programming:**\n",
                "- Thread coalescing is critical: reorganize memory layout (NHWC) so consecutive threads access consecutive addresses\n",
                "- Shared memory reduces global memory traffic by ~9x for 3x3 convolution (18x18 tile shared by 16x16 threads)\n",
                "- Kernel fusion eliminates intermediate buffers and reduces kernel launch overhead\n",
                "- Constant memory provides fast broadcast access for small read-only data (bias values)\n",
                "\n",
                "**Deep Learning:**\n",
                "- He initialization (std = sqrt(2/fan_in)) prevents gradient explosion in ReLU networks\n",
                "- Gradient clipping (max_norm=1.0) stabilizes training for deep networks\n",
                "- Autoencoder features capture visual patterns but have limited discriminative power compared to supervised learning\n",
                "\n",
                "**Performance Optimization:**\n",
                "- Naive GPU implementation is memory-bandwidth bound, not compute-bound\n",
                "- Diminishing returns: Basic->v1 gives 1.35x, v1->v2 gives only 1.01x (approaching memory bandwidth ceiling)\n",
                "- Profile-guided optimization is essential; assumptions about bottlenecks are often wrong\n",
                "\n",
                "## 4.2 Challenges and Solutions\n",
                "\n",
                "**Challenge 1: Gradient Explosion**\n",
                "- Problem: Training loss became NaN after first few epochs due to exploding gradients.\n",
                "- Solution: Implemented He initialization for weights and gradient clipping with max_norm=1.0.\n",
                "- Lesson: Weight initialization is critical for training stability in deep networks.\n",
                "\n",
                "**Challenge 2: Shared Memory Bank Conflicts**\n",
                "- Problem: Shared memory convolution kernel showed lower-than-expected performance.\n",
                "- Solution: Padded shared memory tile width by 1 to avoid 32-way bank conflicts.\n",
                "- Lesson: Memory access patterns matter as much as reducing total memory accesses.\n",
                "\n",
                "**Challenge 3: Fused Kernel Correctness**\n",
                "- Problem: Fused Conv+ReLU kernel produced different results than separate kernels.\n",
                "- Solution: Created unit tests comparing fused vs unfused outputs; discovered bias addition order issue.\n",
                "- Lesson: Systematic testing is essential when optimizing; never assume correctness."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "# Section 5: Conclusion\n",
                "\n",
                "## 5.1 Summary\n",
                "\n",
                "| Metric | Target | Achieved | Status |\n",
                "|--------|--------|----------|--------|\n",
                "| Training time | < 10 min | ~6.3 min / epochs | Met |\n",
                "| GPU speedup | > 20x | ~220x | Exceeded |\n",
                "| Test accuracy | 60-65% | 60.08% | Met |\n",
                "| Feature extraction | < 20s | ~15s | Met |\n",
                "\n",
                "## 5.2 Key Achievements\n",
                "\n",
                "| Achievement | Value |\n",
                "|-------------|-------|\n",
                "| Maximum speedup achieved | 226x (GPU-v2 vs CPU) |\n",
                "| Best-performing optimization | Shared memory tiling (1.35x over naive) |\n",
                "| Classification accuracy | 60.08% on CIFAR-10 test set |\n",
                "| Technical skills mastered | CUDA kernel optimization, memory hierarchy, profiling |\n",
                "\n",
                "## 5.3 Accomplishments\n",
                "\n",
                "| Component | Status |\n",
                "|-----------|--------|\n",
                "| CIFAR-10 Data Loader | Complete |\n",
                "| CPU Autoencoder Baseline | Complete |\n",
                "| GPU Naive Implementation | Complete |\n",
                "| GPU Opt v1 (Shared Memory) | Complete |\n",
                "| GPU Opt v2 (Kernel Fusion) | Complete |\n",
                "| Feature Extraction Pipeline | Complete |\n",
                "| SVM Integration (ThunderSVM) | Complete |\n",
                "\n",
                "## 5.4 Limitations\n",
                "\n",
                "- Backward pass kernels not fully optimized (forward pass is the focus)\n",
                "- 60% accuracy is significantly lower than supervised end-to-end CNN (~96%)\n",
                "- No multi-GPU support implemented\n",
                "- Fixed batch size (not dynamically tuned per GPU memory)\n",
                "\n",
                "## 5.5 Future Work\n",
                "\n",
                "**Performance Improvements:**\n",
                "- Winograd convolution for 3x3 kernels (reduces multiplications)\n",
                "- Multi-stream training to overlap H2D transfer with computation\n",
                "- FP16 mixed precision training for 2x memory bandwidth\n",
                "\n",
                "**Accuracy Improvements:**\n",
                "- Variational Autoencoder (VAE) for better latent space structure\n",
                "- Supervised fine-tuning after unsupervised pre-training\n",
                "- Alternative classifiers (Random Forest, Neural Network head)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "# Appendix A: Project Structure\n",
                "\n",
                "```text\n",
                "AutoEncoder-CUDA/\n",
                "    checkpoints/              # Saved model weights\n",
                "        encoder.weights       # Pre-trained encoder\n",
                "        svm.bin               # Pre-trained SVM model\n",
                "    data/                     # CIFAR-10 binary files\n",
                "    docs/                     # Documentation\n",
                "    external/                 # Third-party libraries (ThunderSVM)\n",
                "    notebooks/                # Jupyter notebooks\n",
                "    scripts/                  # Build and run scripts\n",
                "    src/                      # Source code\n",
                "        benchmarking/         # Performance timing utilities\n",
                "        config/               # Configuration parsing\n",
                "        cpu/                  # CPU baseline implementation\n",
                "            data/             # Data loading\n",
                "            layers/           # Layer implementations\n",
                "            model/            # Autoencoder class\n",
                "            training/         # Training loop\n",
                "        gpu/                  # CUDA implementation\n",
                "            core/             # Memory management, CUDA utils\n",
                "            inference/        # Feature extraction\n",
                "            kernels/          # CUDA kernels\n",
                "                forward/      # Forward pass kernels\n",
                "                backward/     # Backpropagation kernels\n",
                "            model/            # GPU Autoencoder class\n",
                "            svm/              # SVM wrapper\n",
                "        utils/                # Helper functions\n",
                "```"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "# Appendix B: Profiling (Optional)\n",
                "\n",
                "For detailed kernel-level profiling, use NVIDIA Nsight Systems:\n",
                "\n",
                "```bash\n",
                "nsys profile --stats=true ./build/bin/autoencoder_gpu --gpu-version 2 --epochs 1 --samples 1000\n",
                "```\n",
                "\n",
                "This generates a report showing:\n",
                "- Time spent in each kernel\n",
                "- Memory bandwidth utilization\n",
                "- Kernel occupancy\n",
                "- API call overhead\n",
                "\n",
                "---\n",
                "\n",
                "**End of Report**"
            ]
        }
    ],
    "metadata": {
        "accelerator": "GPU",
        "colab": {
            "gpuType": "T4",
            "provenance": []
        },
        "kernelspec": {
            "display_name": "Python 3",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 0
}
